<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description"
    content="ActLCD: Active Layer-Contrastive Decoding reduces hallucination in LLM generation. Project page, paper, code, results.">
  <meta name="keywords"
    content="ActLCD, Active Layer-Contrastive Decoding, Layer Contrast, Decoding, Hallucination, LLM, SLED, DoLa">
  <title>ActLCD: Active Layer-Contrastive Decoding</title>

  <!-- Fonts & Icons -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <!-- Styles (reusing nerfies layout) -->
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <!-- JS -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <style>
    /* Small utility tweaks for this project */
    .subtitle .tag {
      vertical-align: middle;
    }

    .figure {
      border: 1px solid #eee;
      border-radius: 10px;
      overflow: hidden;
    }

    .figure img {
      display: block;
      width: 100%;
    }

    .metric {
      display: inline-block;
      min-width: 120px;
      padding: 8px 12px;
      border: 1px solid #e5e5e5;
      border-radius: 10px;
      margin: 6px 6px 0 0;
      font-weight: 600;
    }

    .results-table {
      overflow-x: auto;
    }

    .hero.is-light .title,
    .hero.is-light .subtitle {
      color: #222;
    }

    /* News bar */
    .news-bar {
      padding: 0;
      /* keep it tight */
      background: transparent;
      /* we use the notification's bg */
    }

    .news-bar .notification {
      border-radius: 12px;
      border: 1px solid #e8eef6;
      box-shadow: 0 6px 16px rgba(30, 60, 90, 0.06);
    }

    .news-bar .pulse-dot {
      position: relative;
      width: 10px;
      height: 10px;
      border-radius: 50%;
      background: #3e8ed0;
      /* Bulma info color family */
    }

    .news-bar .pulse-dot::after {
      content: "";
      position: absolute;
      inset: 0;
      border-radius: 50%;
      animation: pulse 1.6s ease-out infinite;
      box-shadow: 0 0 0 0 rgba(62, 142, 208, 0.6);
    }

    @keyframes pulse {
      0% {
        box-shadow: 0 0 0 0 rgba(62, 142, 208, 0.6);
      }

      70% {
        box-shadow: 0 0 0 10px rgba(62, 142, 208, 0);
      }

      100% {
        box-shadow: 0 0 0 0 rgba(62, 142, 208, 0);
      }
    }
  </style>
</head>

<body>

  <!-- Navbar -->
  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="/">
          <span class="icon"><i class="fas fa-home"></i></span>
        </a>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">Projects</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="/AutoLCD/">AutoLCD</a>
            <a class="navbar-item" href="#results">Results</a>
            <a class="navbar-item" href="#bibtex">BibTeX</a>
          </div>
        </div>
      </div>
    </div>
  </nav>

  <!-- Hero -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">ActLCD: Active Layer-Contrastive Decoding Reduces Hallucination in
              LLM Generation</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><a>Hongxiang Zhang</a><sup>1,2</sup>,</span>
              <span class="author-block"><a>Hao Chen</a><sup>2</sup>,</span>
              <span class="author-block"><a>Muhao Chen</a><sup>2</sup>,</span>
              <span class="author-block"><a>Tianyi Zhang</a><sup>1</sup></span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Purdue University,</span>
              <span class="author-block"><sup>2</sup>University of California, Davis</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Paper PDF -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2505.23657" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-file-pdf"></i></span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2505.23657" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="ai ai-arxiv"></i></span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Video/demo placeholders -->
                <span class="link-block">
                  <a href="#" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fab fa-youtube"></i></span>
                    <span>Video (soon)</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/actlcd/ActLCD" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fab fa-github"></i></span>
                    <span>Code (soon)</span>
                  </a>
                </span>
              </div>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Main Figure / Teaser (replace with your actual image) -->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="figure">
          <img src="./static/images/main.png"
            alt="ActLCD main figure: dynamic activation of layer-contrast during decoding for factual, low-hallucination generation.">
        </div>
        <h2 class="subtitle has-text-centered" style="margin-top: 10px;">
          <span class="tag is-info is-light">Main Figure</span>
          The workflow of ActLCD. (1) Next-token selection: ActLCD dynamically apply layer contrasting at each step. (2)
          Sequential-level optimization: By framing decoding as a Markov decision process, ActLCD selectively activates
          layer contrasting to maximize cumulative reward throughout the generation.
        </h2>
      </div>
    </div>
  </section>

  <!-- News Bar -->
  <section class="news-bar" aria-label="Latest news">
    <div class="container is-max-desktop">
      <div class="notification is-info is-light"
        style="margin: 10px 0 0; display: flex; align-items: center; justify-content: center; gap: 12px; flex-wrap: wrap;">
        <span class="pulse-dot" aria-hidden="true"></span>
        <span class="tag is-info is-light is-medium">
          <span class="icon"><i class="fas fa-bullhorn" aria-hidden="true"></i></span>
          <span>News</span>
        </span>
        <span class="has-text-weight-semibold">ActLCD</span>
        <span>has been <strong>accepted</strong> to <strong>EMNLP Mains&nbsp;2025</strong> üéâ</span>
      </div>
    </div>
  </section>

  <!-- Key Results (headline numbers) -->
  <section class="hero is-light is-small" id="results">
    <div class="hero-body">
      <div class="container is-max-desktop has-text-centered">
        <h2 class="title is-3">Headline Improvements</h2>
        <p class="subtitle">Across 5 benchmarks and multiple LLMs</p>
        <div>
          <!-- Replace values with your exact numbers when ready -->
          <span class="metric">TruthfulQA: <strong>+19.81%</strong></span>
          <span class="metric">LongFact F1@128: <strong>+3.30%</strong></span>
          <span class="metric">StrategyQA: <strong>+7.51%</strong></span>
          <span class="metric">GSM8K: <strong>+7.21%</strong></span>
          <span class="metric">Pkg Halluc.: <strong>+9.23%</strong></span>
        </div>
      </div>
    </div>
  </section>

  <!-- Abstract -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Recent decoding methods improve the factuality of large language models (LLMs) by refining how the next
              token is selected during generation. These methods typically operate at the token level, leveraging
              internal representations to suppress superficial patterns. Nevertheless, LLMs remain prone to
              hallucinations, especially over longer contexts. In this paper, we propose <strong>Active
                Layer-Contrastive Decoding (ActLCD)</strong>, a novel decoding strategy that actively decides when to
              apply contrasting layers during generation. By casting decoding as a sequential decision-making problem,
              ActLCD employs a reinforcement learning policy guided by a reward-aware classifier to optimize factuality
              beyond the token level.
            </p>
            <p>
              Our experiments demonstrate that ActLCD surpasses state-of-the-art methods such as SLED and DoLa across
              open-domain, long-form, chain-of-thought, and
              domain-specific code benchmarks.,
              showcasing its effectiveness in mitigating hallucinations in diverse generation scenarios.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Evaluation tables/images (placeholders) -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Evaluation Results</h2>
          <!-- Example: insert result figure images you export from the paper -->
          <div class="figure" style="margin-bottom: 18px;">
            <img src="./static/images/tb1.png"
              alt="Results overview across TruthfulQA, LongFact, StrategyQA, GSM8K, and package hallucination.">
          </div>
          <div class="content has-text-justified">
            <p>Results overview across TruthfulQA, LongFact, StrategyQA, GSM8K, and package hallucination.</p>
          </div>
          <div class="figure" style="margin-bottom: 18px;">
            <img src="./static/images/tb2.png"
              alt="Greedy correctly computes the initial toy count but then ‚Äúforgets‚Äù that value later in its reasoning, resulting in the incorrect answer. Whereas SLED and DoLa misinterpret the toys needed at the beginning, they subsequently build an entire chain of reasoning on this false assumption, resulting in a significantly incorrect answer. This exemplifies a phenomenon known as ‚Äúhallucination nowballing‚Äù, where early mistakes cascade into increasingly severe errors. Such missteps may be due to the side effect of layer contrasting that forces LLMs to interpret longer sentences, potentially leading to fundamental misunderstandings. In contrast, ActLCD selectively activates layer contrasting to leverage latent knowledge in deep layers, fostering a coherent logical thought chain that yields the affirmative answer.">
          </div>
          <div class="content has-text-justified">
            <p>Greedy correctly computes the initial toy count but then ‚Äúforgets‚Äù that value later in its reasoning,
              resulting in the incorrect answer. Whereas SLED and DoLa misinterpret the toys needed at the beginning,
              they subsequently build an entire chain of reasoning on this false assumption, resulting in a
              significantly incorrect answer. This exemplifies a phenomenon known as ‚Äúhallucination nowballing‚Äù, where
              early mistakes cascade into increasingly severe errors. Such missteps may be due to the side effect of
              layer contrasting that forces LLMs to interpret longer sentences, potentially leading to fundamental
              misunderstandings. In contrast, ActLCD selectively activates layer contrasting to leverage latent
              knowledge in deep layers, fostering a coherent logical thought chain that yields the affirmative answer.
            </p>
          </div>
          <div class="figure" style="margin-bottom: 18px;">
            <img src="./static/images/tb3.png"
              alt="ActLCD significantly reduced package hallucination in both Python and JavaScript. A key challenge in this benchmark is that models must generate multiple package names, where one hallucinated package can compromise the entire response. ActLCD‚Äôs dynamic contrastive mechanism is especially beneficial for this context.">
          </div>
          <div class="content has-text-justified">
            <p>ActLCD significantly reduced package hallucination in both Python and JavaScript. A key challenge in this
              benchmark is that models must generate multiple package names, where one hallucinated package can
              compromise the entire response. ActLCD‚Äôs dynamic contrastive mechanism is especially beneficial for this
              context.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- Related Links -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Related Links</h2>
          <div class="content has-text-justified">
            <p>Representative decoding and factuality works:</p>
            <ul>
              <li><a href="https://arxiv.org/abs/2309.17453">DoLa: Decoding by Contrasting Layers</a></li>
              <li><a href="https://arxiv.org/abs/2406.06218">SLED: Steering LLM Decoding with Layer-wise Evidence</a>
              </li>
              <li><a href="https://arxiv.org/abs/2109.07958">Contrastive Decoding (Li et al.)</a></li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- BibTeX -->
  <section class="section" id="bibtex">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{zhang2025active,
  title={Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation},
  author={Zhang, Hongxiang and Chen, Hao and Chen, Muhao and Zhang, Tianyi},
  journal={arXiv preprint arXiv:2505.23657},
  year={2025}
}</code></pre>
    </div>
  </section>

  <!-- Footer -->
  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is adapted from open-source templates at <a
                href="https://github.com/nerfies/nerfies.github.io">nerfies</a>,
              and content you may reuse under a
              <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons
                Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              If you reuse this site, please include proper attribution and remove any third-party analytics you do not
              intend to use.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>